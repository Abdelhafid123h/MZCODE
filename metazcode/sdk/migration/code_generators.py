#!/usr/bin/env python3
"""
Platform-specific code generators for SSIS to modern data platform migration.

This module provides code generators that convert SSIS graph metadata with SQL semantics
into platform-specific code for Spark, dbt, Azure Data Factory, and Python/Pandas.
"""

import json
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class TargetPlatform(str, Enum):
    """Supported target platforms for migration."""
    SPARK = "spark"
    DBT = "dbt" 
    AZURE_DATA_FACTORY = "adf"
    PANDAS = "pandas"
    DATABRICKS = "databricks"
    SNOWFLAKE = "snowflake"

@dataclass
class MigrationContext:
    """Context information for migration code generation."""
    package_name: str
    source_platform: str = "SSIS"
    target_platform: TargetPlatform = TargetPlatform.SPARK
    database_mapping: Optional[Dict[str, str]] = None
    schema_mapping: Optional[Dict[str, str]] = None
    connection_config: Optional[Dict[str, Any]] = None
    naming_conventions: Optional[Dict[str, str]] = None

@dataclass 
class GeneratedCode:
    """Container for generated migration code."""
    platform: TargetPlatform
    code: str
    dependencies: List[str]
    comments: List[str]
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "platform": self.platform.value,
            "code": self.code,
            "dependencies": self.dependencies,
            "comments": self.comments,
            "metadata": self.metadata
        }

class CodeGenerator(ABC):
    """Abstract base class for platform-specific code generators."""
    
    def __init__(self, context: MigrationContext):
        """Initialize code generator with migration context."""
        self.context = context
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    @abstractmethod
    def generate_from_sql_semantics(self, sql_semantics: Dict[str, Any]) -> GeneratedCode:
        """
        Generate platform-specific code from SQL semantics metadata.
        
        Args:
            sql_semantics: Parsed SQL semantics from enhanced parser
            
        Returns:
            GeneratedCode object with platform-specific implementation
        """
        pass
    
    def _normalize_table_name(self, table_name: str) -> str:
        """Normalize table name according to platform conventions."""
        # Remove brackets and schema prefixes for base name
        clean_name = table_name.replace('[', '').replace(']', '')
        if '.' in clean_name:
            parts = clean_name.split('.')
            return parts[-1]  # Use table name without schema
        return clean_name
    
    def _apply_naming_convention(self, name: str, convention_type: str = "table") -> str:
        """Apply naming conventions based on target platform."""
        if not self.context.naming_conventions:
            return name
        
        convention = self.context.naming_conventions.get(convention_type, "snake_case")
        
        if convention == "snake_case":
            # Convert to snake_case
            import re
            name = re.sub('([A-Z]+)', r'_\1', name).lower()
            return name.strip('_')
        elif convention == "camel_case":
            # Convert to camelCase
            words = name.split('_')
            return words[0].lower() + ''.join(word.capitalize() for word in words[1:])
        elif convention == "pascal_case":
            # Convert to PascalCase
            return ''.join(word.capitalize() for word in name.split('_'))
        
        return name

class SparkCodeGenerator(CodeGenerator):
    """Generate PySpark DataFrame code from SSIS SQL semantics."""
    
    def generate_from_sql_semantics(self, sql_semantics: Dict[str, Any]) -> GeneratedCode:
        """Generate Spark DataFrame operations."""
        
        dependencies = [
            "from pyspark.sql import SparkSession, DataFrame",
            "from pyspark.sql.functions import col, lit, when, coalesce"
        ]
        
        comments = [
            f"# Generated Spark code for {self.context.package_name}",
            f"# Original SQL: {sql_semantics.get('original_query', 'N/A')[:100]}...",
            "# Migration generated by MetaZCode"
        ]
        
        # Extract components
        tables = sql_semantics.get('tables', [])
        joins = sql_semantics.get('joins', [])
        columns = sql_semantics.get('columns', [])
        
        code_lines = []
        
        # Generate DataFrame loading
        code_lines.append("# Load source DataFrames")
        for table in tables:
            table_name = self._normalize_table_name(table['name'])
            df_name = f"df_{table_name.lower()}"
            alias = table.get('alias', table_name.lower())
            
            code_lines.append(f"{df_name} = spark.table('{table_name}')")  # Assuming table catalog
            if alias != table_name.lower():
                code_lines.append(f"{df_name} = {df_name}.alias('{alias}')")
        
        code_lines.append("")
        
        # Generate JOIN operations
        if joins:
            code_lines.append("# Join operations")
            result_df = None
            
            for i, join in enumerate(joins):
                left_table = self._normalize_table_name(join['left_table']['name'])
                right_table = self._normalize_table_name(join['right_table']['name'])
                left_alias = join['left_table'].get('alias', left_table.lower())
                right_alias = join['right_table'].get('alias', right_table.lower())
                
                # Parse join condition
                condition = join.get('condition', '')
                spark_condition = self._convert_join_condition_to_spark(condition, left_alias, right_alias)
                
                join_type = join.get('join_type', 'INNER JOIN').replace(' JOIN', '').lower()
                
                if i == 0:
                    code_lines.append(f"result_df = df_{left_table.lower()}.alias('{left_alias}') \\")
                    code_lines.append(f"    .join(df_{right_table.lower()}.alias('{right_alias}'), \\")
                    code_lines.append(f"          {spark_condition}, \\")
                    code_lines.append(f"          '{join_type}')")
                else:
                    code_lines.append(f"result_df = result_df \\")
                    code_lines.append(f"    .join(df_{right_table.lower()}.alias('{right_alias}'), \\")
                    code_lines.append(f"          {spark_condition}, \\")
                    code_lines.append(f"          '{join_type}')")
        
        else:
            # Single table case
            if tables:
                table_name = self._normalize_table_name(tables[0]['name'])
                code_lines.append(f"result_df = df_{table_name.lower()}")
        
        code_lines.append("")
        
        # Generate SELECT operations
        if columns:
            code_lines.append("# Select columns")
            select_expressions = []
            
            for column in columns:
                expr = column.get('expression', '')
                alias = column.get('alias')
                
                if alias:
                    # Handle aliased columns
                    if '.' in expr:  # table.column format
                        spark_expr = f"col('{expr}')"
                    else:
                        spark_expr = f"col('{expr}')"
                    select_expressions.append(f"{spark_expr}.alias('{alias}')")
                else:
                    # Simple column reference
                    if '.' in expr:
                        select_expressions.append(f"col('{expr}')")
                    else:
                        select_expressions.append(f"col('{expr}')")
            
            if select_expressions:
                code_lines.append("result_df = result_df.select(")
                for i, expr in enumerate(select_expressions):
                    comma = "," if i < len(select_expressions) - 1 else ""
                    code_lines.append(f"    {expr}{comma}")
                code_lines.append(")")
        
        code_lines.append("")
        code_lines.append("# Show results")
        code_lines.append("result_df.show()")
        
        metadata = {
            "table_count": len(tables),
            "join_count": len(joins),
            "column_count": len(columns),
            "estimated_complexity": "medium" if joins else "low"
        }
        
        return GeneratedCode(
            platform=TargetPlatform.SPARK,
            code="\n".join(comments + [""] + dependencies + [""] + code_lines),
            dependencies=dependencies,
            comments=comments,
            metadata=metadata
        )
    
    def _convert_join_condition_to_spark(self, condition: str, left_alias: str, right_alias: str) -> str:
        """Convert SQL JOIN condition to Spark DataFrame condition."""
        # Simple conversion for basic equality joins
        # Example: "p.CategoryID = c.CategoryID" -> "col('p.CategoryID') == col('c.CategoryID')"
        import re
        
        # Handle simple equality conditions
        match = re.match(r'(\w+)\.(\w+)\s*=\s*(\w+)\.(\w+)', condition.strip())
        if match:
            left_table, left_col, right_table, right_col = match.groups()
            return f"col('{left_table}.{left_col}') == col('{right_table}.{right_col}')"
        
        # Fallback: return as string (may need manual adjustment)
        return f"'{condition}'"

class DbtCodeGenerator(CodeGenerator):
    """Generate dbt SQL models from SSIS SQL semantics."""
    
    def generate_from_sql_semantics(self, sql_semantics: Dict[str, Any]) -> GeneratedCode:
        """Generate dbt model SQL."""
        
        dependencies = []  # dbt doesn't have import dependencies like Python
        
        comments = [
            f"-- Generated dbt model for {self.context.package_name}",
            f"-- Original SQL: {sql_semantics.get('original_query', 'N/A')[:100]}...",
            "-- Migration generated by MetaZCode",
            ""
        ]
        
        # Extract components
        tables = sql_semantics.get('tables', [])
        joins = sql_semantics.get('joins', [])
        columns = sql_semantics.get('columns', [])
        
        code_lines = []
        
        # Generate SELECT clause
        if columns:
            code_lines.append("SELECT")
            for i, column in enumerate(columns):
                expr = column.get('expression', '')
                alias = column.get('alias')
                comma = "," if i < len(columns) - 1 else ""
                
                if alias:
                    code_lines.append(f"    {expr} AS {alias}{comma}")
                else:
                    code_lines.append(f"    {expr}{comma}")
        else:
            code_lines.append("SELECT *")
        
        code_lines.append("")
        
        # Generate FROM clause
        if tables:
            main_table = tables[0]
            table_name = self._normalize_table_name(main_table['name'])
            alias = main_table.get('alias', '')
            
            code_lines.append(f"FROM {{{{ ref('{table_name.lower()}') }}}} {alias}")
        
        # Generate JOIN clauses
        for join in joins:
            right_table = join['right_table']
            table_name = self._normalize_table_name(right_table['name'])
            alias = right_table.get('alias', '')
            join_type = join.get('join_type', 'INNER JOIN')
            condition = join.get('condition', '')
            
            code_lines.append(f"{join_type} {{{{ ref('{table_name.lower()}') }}}} {alias}")
            code_lines.append(f"    ON {condition}")
        
        metadata = {
            "model_type": "transformation",
            "materialized": "table",  # Default materialization
            "table_count": len(tables),
            "join_count": len(joins),
            "column_count": len(columns)
        }
        
        # Add dbt configuration
        config_lines = [
            "{{ config(",
            "    materialized='table',",
            f"    description='Migrated from SSIS package: {self.context.package_name}'",
            ") }}"
        ]
        
        full_code = "\n".join(comments + config_lines + [""] + code_lines)
        
        return GeneratedCode(
            platform=TargetPlatform.DBT,
            code=full_code,
            dependencies=dependencies,
            comments=comments,
            metadata=metadata
        )

class PandasCodeGenerator(CodeGenerator):
    """Generate Python/Pandas code from SSIS SQL semantics."""
    
    def generate_from_sql_semantics(self, sql_semantics: Dict[str, Any]) -> GeneratedCode:
        """Generate Pandas DataFrame operations."""
        
        dependencies = [
            "import pandas as pd",
            "import numpy as np",
            "from typing import Dict, List, Optional"
        ]
        
        comments = [
            f"# Generated Pandas code for {self.context.package_name}",
            f"# Original SQL: {sql_semantics.get('original_query', 'N/A')[:100]}...",
            "# Migration generated by MetaZCode"
        ]
        
        # Extract components
        tables = sql_semantics.get('tables', [])
        joins = sql_semantics.get('joins', [])
        columns = sql_semantics.get('columns', [])
        
        code_lines = []
        
        # Generate DataFrame loading
        code_lines.append("# Load source DataFrames")
        code_lines.append("# TODO: Replace with actual data loading logic")
        for table in tables:
            table_name = self._normalize_table_name(table['name'])
            df_name = f"df_{table_name.lower()}"
            code_lines.append(f"{df_name} = pd.read_sql(\"SELECT * FROM {table_name}\", connection)")
        
        code_lines.append("")
        
        # Generate JOIN operations
        if joins:
            code_lines.append("# Join operations")
            
            for i, join in enumerate(joins):
                left_table = self._normalize_table_name(join['left_table']['name'])
                right_table = self._normalize_table_name(join['right_table']['name'])
                condition = join.get('condition', '')
                
                # Parse join condition for merge keys
                left_key, right_key = self._parse_join_keys(condition)
                join_type = self._convert_join_type_to_pandas(join.get('join_type', 'INNER JOIN'))
                
                if i == 0:
                    code_lines.append(f"result_df = pd.merge(")
                    code_lines.append(f"    df_{left_table.lower()},")
                    code_lines.append(f"    df_{right_table.lower()},")
                    code_lines.append(f"    left_on='{left_key}',")
                    code_lines.append(f"    right_on='{right_key}',")
                    code_lines.append(f"    how='{join_type}',")
                    code_lines.append(f"    suffixes=('_left', '_right')")
                    code_lines.append(")")
                else:
                    code_lines.append(f"result_df = pd.merge(")
                    code_lines.append(f"    result_df,")
                    code_lines.append(f"    df_{right_table.lower()},")
                    code_lines.append(f"    left_on='{left_key}',")
                    code_lines.append(f"    right_on='{right_key}',")
                    code_lines.append(f"    how='{join_type}',")
                    code_lines.append(f"    suffixes=('', '_right')")
                    code_lines.append(")")
        else:
            # Single table case
            if tables:
                table_name = self._normalize_table_name(tables[0]['name'])
                code_lines.append(f"result_df = df_{table_name.lower()}.copy()")
        
        code_lines.append("")
        
        # Generate column selection
        if columns:
            code_lines.append("# Select and rename columns")
            column_list = []
            rename_dict = {}
            
            for column in columns:
                expr = column.get('expression', '')
                alias = column.get('alias')
                
                if alias and alias != expr:
                    column_list.append(expr)
                    rename_dict[expr] = alias
                else:
                    column_list.append(expr)
            
            code_lines.append(f"selected_columns = {column_list}")
            code_lines.append("result_df = result_df[selected_columns]")
            
            if rename_dict:
                code_lines.append(f"result_df = result_df.rename(columns={rename_dict})")
        
        code_lines.append("")
        code_lines.append("# Display results")
        code_lines.append("print(result_df.head())")
        
        metadata = {
            "table_count": len(tables),
            "join_count": len(joins),
            "column_count": len(columns),
            "requires_database_connection": True
        }
        
        return GeneratedCode(
            platform=TargetPlatform.PANDAS,
            code="\n".join(comments + [""] + dependencies + [""] + code_lines),
            dependencies=dependencies,
            comments=comments,
            metadata=metadata
        )
    
    def _parse_join_keys(self, condition: str) -> Tuple[str, str]:
        """Parse join condition to extract left and right keys."""
        import re
        
        # Handle simple equality conditions: "table1.col1 = table2.col2"
        match = re.match(r'(\w+)\.(\w+)\s*=\s*(\w+)\.(\w+)', condition.strip())
        if match:
            left_table, left_col, right_table, right_col = match.groups()
            return left_col, right_col
        
        # Fallback: assume same column name
        return "id", "id"
    
    def _convert_join_type_to_pandas(self, join_type: str) -> str:
        """Convert SQL JOIN type to Pandas merge type."""
        join_type = join_type.upper()
        if "LEFT" in join_type:
            return "left"
        elif "RIGHT" in join_type:
            return "right"
        elif "FULL" in join_type or "OUTER" in join_type:
            return "outer"
        else:
            return "inner"

class MigrationCodeGeneratorFactory:
    """Factory for creating platform-specific code generators."""
    
    @staticmethod
    def create_generator(platform: TargetPlatform, context: MigrationContext) -> CodeGenerator:
        """Create appropriate code generator for target platform."""
        
        if platform == TargetPlatform.SPARK:
            return SparkCodeGenerator(context)
        elif platform == TargetPlatform.DBT:
            return DbtCodeGenerator(context)
        elif platform == TargetPlatform.PANDAS:
            return PandasCodeGenerator(context)
        elif platform == TargetPlatform.DATABRICKS:
            # Databricks uses Spark API
            return SparkCodeGenerator(context)
        else:
            raise ValueError(f"Unsupported target platform: {platform}")

def generate_migration_code(
    sql_semantics: Dict[str, Any],
    target_platform: TargetPlatform,
    context: MigrationContext
) -> GeneratedCode:
    """
    Generate migration code for specified target platform.
    
    Args:
        sql_semantics: Parsed SQL semantics from enhanced parser
        target_platform: Target platform for migration
        context: Migration context with configuration
        
    Returns:
        GeneratedCode object with platform-specific implementation
    """
    generator = MigrationCodeGeneratorFactory.create_generator(target_platform, context)
    return generator.generate_from_sql_semantics(sql_semantics)

def generate_migration_code_for_all_platforms(
    sql_semantics: Dict[str, Any],
    context: MigrationContext,
    platforms: Optional[List[TargetPlatform]] = None
) -> Dict[str, GeneratedCode]:
    """
    Generate migration code for multiple target platforms.
    
    Args:
        sql_semantics: Parsed SQL semantics from enhanced parser
        context: Migration context with configuration
        platforms: List of target platforms (defaults to all supported)
        
    Returns:
        Dictionary mapping platform names to GeneratedCode objects
    """
    if platforms is None:
        platforms = [TargetPlatform.SPARK, TargetPlatform.DBT, TargetPlatform.PANDAS]
    
    results = {}
    
    for platform in platforms:
        try:
            generated_code = generate_migration_code(sql_semantics, platform, context)
            results[platform.value] = generated_code
        except Exception as e:
            logger.error(f"Failed to generate code for {platform.value}: {e}")
            # Create error placeholder
            results[platform.value] = GeneratedCode(
                platform=platform,
                code=f"# Error generating code: {e}",
                dependencies=[],
                comments=[f"# Failed to generate {platform.value} code"],
                metadata={"error": str(e)}
            )
    
    return results